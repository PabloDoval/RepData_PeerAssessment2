```{r CaptureDate, echo = FALSE, message = FALSE, tidy = FALSE}
## This one is left with echo = FALSE due to aesthethics motives. This is 
## just so the document automatically updates the Date to the last compile
## date.
captureDate <- Sys.Date()
```
---
title: "Study on Human and Economic Impact of Major Weather Events"
author: Pablo Doval
date: `r captureDate`
output: 
  html_document:
    keep_md: true
    theme: united
    highlight: neon
---

# Study on Human and Economic Impact of Major Weather Events

## Synopsis

bla bla

## Pre-requisites for Data Analysis

If the reader decides to go ahead and reproduce the analysis done on this study, the following pre-requisites must be met:

* The data file must be downloaded and available in R's Working Directory.
* R version is 3.1.2 (others might work, but this was developed with this specific version)
* Package ggplot2 must be installed.
* Package dplyr must be installed.
* Package gridExtra must be installed.
* Package R.utils must be installed.


## Data Processing

The Data Set is available at the following [URL](https://d396qusza40orc.cloudfront.net/repdata%2Fdata%2FStormData.csv.bz2), so the load into R should be automated to cater for any possible modification on the source data set. Since the original data file is stored in a compressed bzip2 file, we will need to uncompress it right after download in order to be able to load into a data frame; we will use the library "R.utils" for that.

```{r LoadData, echo = TRUE, cache = TRUE, message = FALSE, tidy = FALSE}
library(R.utils)

## Download the Data Set into the current working directory
if (!file.exists("./repdata%2Fdata%2FStormData.csv.bz2")) 
{
    fileUrl <- "http://d396qusza40orc.cloudfront.net/repdata%2Fdata%2FStormData.csv.bz2"
    download.file(fileUrl, destfile = "./stormData.csv.bz2")
} else 
{
    message("The Data Set has already been loaded previously. Please, delete it manually if you want to refresh.")
}

## Uncompress the file in the current working directory
if (!file.exists("./stormData.csv")) 
{
    bunzip2("./stormData.csv.bz2")
} else 
{
    message("The Data Set has already been loaded previously. Please, delete it manually if you want to refresh.")
}

## Now we can load the source data
sourceData <- read.csv("./stormData.csv")

## Basic statistics on the data source
numObs <- nrow(sourceData)
numVar <- ncol(sourceData)

```

*NOTE: Due to the size of the data set, the cache has been enabled on this knitr document.*

This data set contains **`r numObs`** observations, with **`r numVar`** variables each.

### Geographycal Filtering

The analysis will be performed over the 50 states of the USA, so the data set will be filtered accordingly using R's default *state.abb* data set:

```{r FilterByState, echo = TRUE, cache = TRUE, message = FALSE, tidy = FALSE}
library(dplyr)

## Filter out the rows that are not included in state.abb data set
filteredByState <- sourceData %>% filter(STATE %in% state.abb) 
numFilteredObs <- nrow(filteredByState)
```

The resulting data set, once filtered by the correct states, is comprised of **`r numFilteredObs`**, ouf of the total of *`r numObs`* in the original data set.


### Variables of Interest

Since this study will focus on data related to the injuries, casualties and property damage, the relevant columns will be filtered from the data set, retaining the following ones:

* Date of the Begining of the Event
* Event Type
* Fatalities
* Injuries
* Property Damage
* Property Damage Exponent
* Crop Damage
* Crop Damage Exponent

```{r FilterData, echo = TRUE, cache = TRUE, message = FALSE, tidy = FALSE}
library(dplyr)

## Create a projection with the specified columns
projection <- filteredByState %>%
              select(BGN_DATE, EVTYPE, FATALITIES, INJURIES,  
                     PROPDMG, PROPDMGEXP, CROPDMG, CROPDMGEXP)

## Rename the columns
names(projection) <- c("BeginDate", "EventType", "Fatalities", "Injuries", "PropertyDamage", 
                       "PropertyDamageExponent", "CropDamage", "CropDamageExponent")

## Convert the BeginDate to a proper Data variable
projection$BeginDate <- as.Date(projection$BeginDate, format = "%m/%d/%Y")
```

### Date Filtering

The analysis of the data set reveals that only sparse events are recorded prior to 1993, with most of the event categories being tracked only on that date and onwards. In order to do the most accurrate analysis possible, the data set will be filtered with events starting in 1993 up to 2011.

```{r DateFiltering, echo = TRUE, message = FALSE, tidy = FALSE}
library(dplyr)
## Filter all events prior to 1st Jan 1993
events <- projection %>% filter(BeginDate >= "1993/01/01")
```

### Compute Property Damage cost

Using the description at the chapter 2.7 in the NOAA document [Storm Data Preparation](https://d396qusza40orc.cloudfront.net/repdata%2Fpeer2_doc%2Fpd01016005curr.pdf), the exponent on the Property Damange can be computed:

```{r PropertyDamageExponent, echo = TRUE, message = FALSE, tidy = FALSE}

## Exponent mapping to U.S. Dollars
events$PropertyDamageExponent <- toupper(as.character(events$PropertyDamageExponent))
events$PropertyDamageExponent[grepl("H", events$PropertyDamageExponent)] <- 100
events$PropertyDamageExponent[grepl("K", events$PropertyDamageExponent)] <- 1000
events$PropertyDamageExponent[grepl("M", events$PropertyDamageExponent)] <- 1000000
events$PropertyDamageExponent[grepl("B", events$PropertyDamageExponent)] <- 1000000000
events$PropertyDamageExponent[grepl("2", events$PropertyDamageExponent)] <- 100
events$PropertyDamageExponent[grepl("3", events$PropertyDamageExponent)] <- 1000
events$PropertyDamageExponent[grepl("4", events$PropertyDamageExponent)] <- 10000
events$PropertyDamageExponent[grepl("5", events$PropertyDamageExponent)] <- 100000
events$PropertyDamageExponent[grepl("6", events$PropertyDamageExponent)] <- 1000000
events$PropertyDamageExponent[grepl("7", events$PropertyDamageExponent)] <- 10000000
events$PropertyDamageExponent[grepl("8", events$PropertyDamageExponent)] <- 100000000

## Conversion to integer values
events$PropertyDamageExponent <- as.integer(events$PropertyDamageExponent)
```

With the exponent as an integer value, we can now proceed to compute the adjusted cost on property damage:

```{r PropertyDamage, echo = TRUE, message = FALSE, tidy = FALSE}
library(dplyr)

## Compute the adjusted property damage
events <- mutate(events, TotalPropertyDamageCost = PropertyDamage * PropertyDamageExponent)
```

### Compute Crop Damage cost

Similarly to the Property Damange above, the description in the chapter 2.7 in the NOAA document [Storm Data Preparation](https://d396qusza40orc.cloudfront.net/repdata%2Fpeer2_doc%2Fpd01016005curr.pdf), allows to compute the exponent of the Crop Damage:

```{r CropDamageExponent, echo = TRUE, message = FALSE, tidy = FALSE}

## Exponent mapping to U.S. Dollars
events$CropDamageExponent <- toupper(as.character(events$CropDamageExponent))
events$CropDamageExponent[grepl("H", events$CropDamageExponent)] <- 100
events$CropDamageExponent[grepl("K", events$CropDamageExponent)] <- 1000
events$CropDamageExponent[grepl("M", events$CropDamageExponent)] <- 1000000
events$CropDamageExponent[grepl("B", events$CropDamageExponent)] <- 1000000000
events$CropDamageExponent[grepl("2", events$CropDamageExponent)] <- 100
events$CropDamageExponent[grepl("3", events$CropDamageExponent)] <- 1000
events$CropDamageExponent[grepl("4", events$CropDamageExponent)] <- 10000
events$CropDamageExponent[grepl("5", events$CropDamageExponent)] <- 100000
events$CropDamageExponent[grepl("6", events$CropDamageExponent)] <- 1000000
events$CropDamageExponent[grepl("7", events$CropDamageExponent)] <- 10000000
events$CropDamageExponent[grepl("8", events$CropDamageExponent)] <- 100000000

## Conversion to integer values
events$CropDamageExponent <- as.integer(events$CropDamageExponent)
```

With the exponent as an integer value, we can now proceed to compute the adjusted cost on property damage:

```{r CropDamage, echo = TRUE, message = FALSE, tidy = FALSE}
library(dplyr)

## Compute the adjusted crop damage
events <- mutate(events, TotalCropDamageCost = CropDamage * CropDamageExponent)
```


### Data Quality Issues

An very basic exploratory data analysis exercise on the data reveals some issues with the quality of the NOAA Storm Data Set used on this analysis that needs to be addressed:

```{r ExploratoryAnalysis, echo = TRUE, message = FALSE, tidy = FALSE}
library(dplyr)

## Compute the list of different types and count the total rows
eventTypes <- distinct(projection, EventType)
numDifferentEvents <- nrow(distinct(projection, EventType))
```

As can be seen, there is a total of **`r numDifferentEvents`** different types of events, which seems quite a large dimensionality to handle when doing our analysis. Digging deeped in the types of events on the list, we can notice immediately that there is a large ammount of duplicates, typographic errors, and general data quality issues, as can be seen in this example searching for Coastal related elements, which reveals quite a surprisingly high number of elements related to Coastal Flooding:

```{r EventTypeQualityIssuesExample, echo = TRUE, message = FALSE, tidy = FALSE}
library(dplyr)

## Filter all Coast* related event types as example
dataQualityIssue <- eventTypes %>% filter(grepl("Coas*", EventType, ignore.case = TRUE)) %>% select(EventType)
dataQualityIssue
```

In order to come to a more usable data set and to get rid of some of the data quality issues, the dimensionality of the EventType column will be reduced via gruping of similar events. In order to select the groups, the NOAA document [Storm Data Preparation](https://d396qusza40orc.cloudfront.net/repdata%2Fpeer2_doc%2Fpd01016005curr.pdf) has been used as a reference; in particular, the table 2.1.1 (Storm Data Event Table):

```{r EventTypeDimensionalityReduction, echo = TRUE, message = FALSE, tidy = FALSE}
library(dplyr)

## Manual process of dimensionality reduction by grouping of events with
## similar text using RegEx.
events$EventType <- toupper(events$EventType) 
events$EventType[grepl("FLOOD", events$EventType)] <- "Flood Events"
events$EventType[grepl("WIND", events$EventType)] <- "Wind Events"
events$EventType[grepl("HEAT", events$EventType)] <- "Extreme Heat"
events$EventType[grepl("COLD", events$EventType)] <- "Cold Weather Events"
events$EventType[grepl("FIRE", events$EventType)] <- "Wild Fire"
events$EventType[grepl("RAIN", events$EventType)] <- "Heavy Rain Effects"
events$EventType[grepl("TORNADO", events$EventType)] <- "Tornados"
events$EventType[grepl("AVAL", events$EventType)] <- "Avalanches"
events$EventType[grepl("HURRICANE", events$EventType)] <- "Hurricanes"
events$EventType[grepl("TROPICAL", events$EventType)] <- "Tropical Storms"
events$EventType[grepl("CURRENT", events$EventType)] <- "Rip Current"
events$EventType[grepl("WINTER", events$EventType)] <- "Generic Winger Weather Events"

```


## Results

Using the NOAA's Storm Data Set - processed as described above - we can evaluate the impact of the different meteorological events on the population, 

With regards to the time evolucion, it seems there is an improvement...

In terms of economic impact of the different 

```{r , echo = TRUE, message = FALSE, tidy = FALSE}
library(ggplot2)
library(dplyr)
library(tidyr)

## Summarize by PropertyDamange and CropDamage
costAnalysis <- events %>%
                select(EventType, TotalCropDamageCost, TotalPropertyDamageCost) %>%
                group_by(EventType) %>%
                summarise(Property = sum(TotalPropertyDamageCost), 
                          Crop = sum(TotalCropDamageCost),
                          Total = sum(TotalPropertyDamageCost + TotalCropDamageCost)) %>%
                arrange(desc(Total))

## Unpivot and get Top 10
costAnalysis <- costAnalysis %>% 
                gather(Category, Cost, -EventType)

costAnalysis2 <- top_n(costAnalysis, 10, Cost)

## Plot the economical impact analysis
ggplot(data = costAnalysis2, aes(x = EventType, y = Cost, fill = Category)) + 
  geom_bar(stat="identity") +
  ggtitle("Titulo") +
  xlab("Weather Event Type") +
  ylab("Billions of Dollars") +
  labs(fill="Damage Type")
}